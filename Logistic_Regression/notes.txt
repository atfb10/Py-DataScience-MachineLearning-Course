Adam Forestier 
April 15, 2023

Logistic Regression
    - Classification algorithm to predict categorical variables by transforming a Linear function into a logistic one (Sigmoid Function)
        *Example: predict image
            0 street image 
            1 car image 
    - Any continuous target can be converted into categories through discretization
        * Example: Convert house price to range 
            0 house price $0-100k
            1 house price $100-200k
            2 house price $ > 200k
    - Classification algortithms also often produce a probability prediction belonging to a class 
        * Example:
            0: 10% probability
            1: 85% probability
            2: 5% probability
    - Sign of Coefficient
        * Positve Beta indicates in increase in likelihood of belonging to 1 class with increase in associated x feature 
        * Negative Beta indicates a decrease in likelihood of belonging to 1 class with increase in associated x feature
    - Magnitude of Coefficient
        * Comparing magnitudes of coefficients against each other can lead to insight over which features have the strongest effect on prediction output
            > We can use odds ration essentially comparing magnitudes of coefficients against each other
            > Do this to see which features have stronger prediction power 
            > Harder to directly interpret magnitude of Beta directly, especially when we could have discrete and continuous x feature values 
            > In terms of fitting coefficients, one does it in terms of log odds
    - Maximum Likelihood
        * go from log odds back to probability
        * Measure the likelihoods of probabilities 
        * Likelihood = product of probabilities of belonging to a class 1
            > ratio until hitting .5 probability of belonging to class 1
            > After which, it is 1 - the ratio of of probability of class 1; because that will give the ratio of hitting class 0
            > Take natural log of each  
                > example .9, .8, .65, .3, .2 = ln(.9) * ln(.8) * ln(.65) * ln(1 - .3), * ln(1 - .2) = likelihood
    - Testing model accuracy
        * Choose best coefficient values in log odds terms that creates maximum likelihood
        * While we are trying to maximize the likelihood, we still need something to minimize, since the computer's gradient descent methods can only search for minimums
    - Classification model general data exploratation steps 
        * statistics
            - value count of y label
            - correlation of x features to y label 
            - description of whole data set
        * visualization
            - boxplot of each x feature on the label (somewhat covered by pairplot. Just shows distribution more explicitly)
            - pairplot on data with hue set to the label: BEST! 
            - Heatmap of correlation: Also Amazing!
    - Classification
        * Confusion Matrix
            > True Positive
            > False Positive 
            > False Negative
            > True Negative
        * Accuracy
            > Taken by adding up true positives and true negatives then dividing by the total
            > Accuracy Paradox 
                # Any classifier dealing with imbalanced classes has to confront the issue of the accuracy paradox 
                # Imbalanced classes will always results in a distorted accuracy reflecting better performance than is truly warranted
                    ^ Examples of imbalanced classes
                        * Medication conditions can affect small portions of the population 
                        * Fraud (real vs fraudelent credit card usage)
                # If a class is only a small percentage (n%), then a classifier that always predicts the majority class will always have an accuracy of (1-n)
                    ^ Example: 95/100 people are healthy, 5/100 are infected 
                        * This means that "accuracy" is 95%! Even if every individual is predicted as healhty. This is not good!
                        * Need precision, recall and f1-score for more balanced performance metrics
        * Precision
        * Recall 
        * f1-score
        * ROC Curves
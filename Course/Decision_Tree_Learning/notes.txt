Adam Forestier
April 24, 2024

# NOTE: SO IMPORTANT - in SciKit-Learn - Decision trees CANNOT take in multicategorical data as strings for features! Must make dummies!
# NOTE: CAN keep labels as strings
# NOTE: DO NOT NEED TO DO FEATURE SCALING FOR DECISION TREES!

Decision Tree Learning - refers to the statistical modeling that uses a form of decision trees, where node splits are decided on information metric
    * Relies on information to split on nodes; information must be mathmatical (duh)

Node Impurity - 

Classification & Regression Tree (CART Algorithms)
    * huge leap forward in practical usage of decision tree algorithm
    * CART based methods are standard; used by SciKit-Learn
    * Concepts:
        > Cross validation of trees
        > Pruning trees
        > Surrogate splits
        > Variable importance scores
        > Search for linear splits

Pruning - Cutting off leaf nodes and making terminal node higher
    * Takes care of overfitting

Gini Impurity - Most common information measurement for decision trees. A mathmatical measurement of how "pure" the information in a dataset is
    * In regards to Classification, this measurement can be thought of as "class uniformity"
    * Plotted out, Gini Impurity is a curve with a y-axis value of 0 for both a 0% and 100% probability of belonging to a class
    * The highest Gini Impurity you will have, is .5, a 50% chance of belonging to class
    * Formula - (class total / total) * (1 - (class total / total)) -> Do this for each class, then take the sum of results!
        > Example A: 2 class red, 2 class blue
        > class red: (2/4)(1 - (2/4)) = .25
        > class blue: (2/4)(1 - (2/4)) = .25
        > Gini Impurity = .25 + .25 = .5 (This is highest possible Gini Impurity -> it cannot be higher than .5)

        > Example B: 1 class red, 3 class blue
        > class red: (1/4)(1 - (1/4)) = .1875
        > class blue: (3/4)(1 - (3/4)) = .1875
        > Gini Impurity = .1875 + .1875 = .375
    * WHEN DOING FOR DECISION TREE, you take the WEIGHTED AVERAGE of leaf node Gini Impurities to get the Gini Impurity for the feature 

Constructing Decision Trees with Gini Impurity
    * We use Gini Impurity to decide on data split values to separate out classes
    * We want to MINIMIZE Gini Impurity at leaf nodes 
        > Minimized impurity at leaf nodes means we are seperating out classes effectively
    * Use feature with the lowest Gini Impurity as the root node
    * Issues to consider:
        > Multiple features
        > Continuous features
            1. Sort data 
            2. Calculate potential split values - Take averages between rows as values and split
            3. Calculate gini impurity for each split
            4. Use gini impurity w/ lowest impurity split value as root node
        > Multi-categorical features
            1. Calculate gini impurity for all combinations
            2. Choose combination that has lowest gini impurity and select this as the root node

Why lowest gini impurity as root?
    * By choosing the feature with lowest resulting gini impurity in its leaf nodes, we are choosing the feature that best splits the data into "pure" classes

Adjusting Parameters
- By using Gini Impurity as a measurement of the effectiveness of a node split, we can perform automatic feature selection 
  by mandating an impurity threshold for an additional feature based split to occur! 
- For overfitted trees, adding a minimum gini impurity decrease will reduce variance & raise bias!
- We can mandate a maximum depth of tree

# Final Note(s):
    * Excellent developments have been made in decision trees
    * Default Decision Tree Learning Model is not used commonly any more, as Random Forests and Boosted are even better!